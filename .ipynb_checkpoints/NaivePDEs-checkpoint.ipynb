{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raroog/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving simple differential equations with naive optimization\n",
    "\n",
    "In this notebook, we investigate how does the optimization of the loss defined by sum of unsupervised minimalization of the equation and by supervised optimization for MSE for the boundary conditions\n",
    "$$Loss(N)=\\sum_i \\left(\\Delta\\Psi(N(x_i))+E\\Psi(N(x_i))\\right)^2 + \\sum_k (\\Psi(N(x_k))-C_k)^2$$\n",
    "works for simple PDEs.\n",
    "\n",
    "This work is a mixture of methods and examples from\n",
    "  1. Lagaris, I., Likas, A., & Fotiadis, D. 1997, Computer Physics Communications,104, 1\n",
    "  1. Shirvany, et. al., \"Numerical solution of the nonlinear Schrodinger equation by feedforward neural networks.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our model by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Solution(tf.keras.models.Model):\n",
    "  def __init__(self, n_i, n_h, n_o=2, activation='sigmoid'):\n",
    "    super(Solution, self).__init__()\n",
    "    \n",
    "    # Dimension of all the layers\n",
    "    self.n_i = n_i\n",
    "    self.n_h = n_h\n",
    "    self.n_o = n_o\n",
    "    \n",
    "    # Shallow network\n",
    "    # Hidden layer\n",
    "    self.hidden_layer = tf.keras.layers.Dense(units=n_h, activation=activation)\n",
    "    # Output layer\n",
    "    self.output_layer = tf.keras.layers.Dense(units=n_o, activation='linear')\n",
    "    \n",
    "  def call(self, X):\n",
    "    # Conversion to a tensor\n",
    "    X = tf.convert_to_tensor(X)\n",
    "    \n",
    "    # Simple Shallow Network Response\n",
    "    response = self.hidden_layer(X)\n",
    "    response = self.output_layer(response)\n",
    "    \n",
    "    response = tf.math.reduce_prod(response, axis=1)\n",
    "    \n",
    "    return response\n",
    "  \n",
    "  def train(self, X, loss_function, epochs, conditions, eigen_value=None, verbose=True,\n",
    "            message_frequency=1, learning_rate=0.1, boundary_multiplier=10,\n",
    "            optimizer_name='Adam'):\n",
    "    \n",
    "    # Checking for the right parameters\n",
    "    if not isinstance(epochs, int) or epochs < 1:\n",
    "      raise Exception('epochs parameter should be a positive integer.')\n",
    "    if not isinstance(message_frequency, int) or message_frequency < 1:\n",
    "      raise Exception(\n",
    "                'message_frequency parameter should be a positive integer.')\n",
    "      \n",
    "    # Choosing the optimizers\n",
    "    optimizer = None\n",
    "    if optimizer_name == 'Adam':\n",
    "      optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'SGD':\n",
    "      optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'Adagrad':\n",
    "      optimizer = tf.keras.optimizers.Adagrad(learning_rate=learning_rate)\n",
    "    \n",
    "    def loss_boundary(network, conditions):\n",
    "      loss = tf.constant(0., shape=(1,), dtype='float64')\n",
    "      for condition in conditions:\n",
    "        X = tf.constant(condition['value'], shape=(1,1), dtype='float64')\n",
    "        boundary_response = None\n",
    "        if condition['type'] == 'dirichlet':\n",
    "          boundary_response = network(X)\n",
    "        elif condition['type'] == 'neuman':\n",
    "          with tf.GradientTape() as tape:\n",
    "            tape.watch(X)\n",
    "            response = network(X)\n",
    "          boundary_response = tape.gradient(response, X)\n",
    "        else:\n",
    "          raise Exception('Wrong type of condition.')\n",
    "        boundary_response = tf.reshape(boundary_response, shape=(-1,))\n",
    "        boundary_value = condition['function'](X)\n",
    "        boundary_value = tf.reshape(boundary_value, shape=(-1,))\n",
    "        loss += (boundary_response - boundary_value) ** 2\n",
    "      loss = boundary_multiplier*tf.math.reduce_sum(loss)\n",
    "      return loss\n",
    "\n",
    "    # Single train step function for the unsupervised equation part\n",
    "    @tf.function\n",
    "    def train_step(X, conditions, eigen_value):\n",
    "      with tf.GradientTape() as tape:\n",
    "        loss = loss_function(self, X, eigen_value)\n",
    "      gradients = tape.gradient(loss, self.trainable_variables)\n",
    "      optimizer.apply_gradients(\n",
    "                  zip(gradients, self.trainable_variables))\n",
    "      with tf.GradientTape() as tape2:\n",
    "        loss = loss_boundary(self, conditions)\n",
    "      gradients = tape2.gradient(loss, self.trainable_variables)\n",
    "      optimizer.apply_gradients(\n",
    "                  zip(gradients, self.trainable_variables))\n",
    "      \n",
    "    # Training for a given number of epochs\n",
    "    for epoch in range(epochs):\n",
    "      train_step(X, conditions, eigen_value)\n",
    "      equation_loss = loss_function(self, X, eigen_value)\n",
    "      boundary_loss = loss_boundary(self, conditions)\n",
    "      if verbose and(epoch+1) % message_frequency == 0:\n",
    "        print(f'Epoch: {epoch+1} Loss equation: \\\n",
    "              {equation_loss.numpy()} \\\n",
    "              Loss boundary: {boundary_loss.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5\n",
    "\n",
    "$\\Delta\\Psi(x, y)=\\exp(-x)\\left(x-2+y^3+6y\\right)$\n",
    "\n",
    "With boundary conditions $\\Psi(0,y)=y^3$, $\\Psi(1, y)=(1+y^3)e^{-1}$, $\\Psi(x, 0)=xe^{-x}$, and $\\Psi(x, 1)=(1+x)e^{-x}$.\n",
    "\n",
    "The PDE will be considered on the domain $(x,y)\\in[0,1]^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "X_train = np.linspace(0, 1, n_samples)\n",
    "Y_train = np.linspace(0, 1, n_samples)\n",
    "X_train, Y_train = np.meshgrid(X_train, Y_train)\n",
    "X_train = X_train.flatten()\n",
    "Y_train = Y_train.flatten()\n",
    "samples_train = np.array([X_train, Y_train]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples_test = 100\n",
    "X_test = np.linspace(0, 1, n_samples_test)\n",
    "Y_test = np.linspace(0, 1, n_samples_test)\n",
    "X_test, Y_test = np.meshgrid(X_test, Y_test)\n",
    "X_test = X_test.flatten()\n",
    "Y_test = Y_test.flatten()\n",
    "samples_test = np.array([X_test, Y_test]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss for this equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diff_loss(network, inputs):\n",
    "  with tf.GradientTape() as tape2:\n",
    "    with tf.GradientTape() as tape:\n",
    "      inputs = tf.convert_to_tensor(inputs)\n",
    "      tape.watch(inputs)\n",
    "      tape2.watch(inputs)\n",
    "      response = network(inputs)  \n",
    "    grads = tape.gradient(response, inputs)\n",
    "  laplace = tape2.gradient(grads, inputs)\n",
    "  two = tf.constant(2, dtype='float64')\n",
    "  loss = tf.square(laplace[:,0] + laplace[:,1]\n",
    "                   - tf.exp(-inputs[:,0])*(inputs[:,0] - two  + inputs[:,1]**3 + inputs[:,1]))\n",
    "  return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
